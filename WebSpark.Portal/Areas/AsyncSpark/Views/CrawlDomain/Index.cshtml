@using WebSpark.HttpClientUtility.Crawler
@model CrawlDomainViewModel

@{
    ViewData["Title"] = "Site Crawler";
    ViewData["Description"] = "Learn about the Site Crawler functionality, its implementation details, and best practices for efficient and reliable crawling.";
    ViewData["Keywords"] = "Site Crawler, Web Crawling, ASP.NET, SignalR, HttpClient, Best Practices";
}

<div class="container pt-5 mt-4">
    <div class="row mb-4">
        <div class="col-12">
            <h1 class="display-6 text-center"><i class="bi bi-search"></i> Crawl Domain</h1>
        </div>
    </div>

    <div class="row">
        <div class="col-md-8 mx-auto">
            <form method="post">
                <div class="mb-3">
                    <label asp-for="StartPath" class="form-label"><i class="bi bi-link-45deg"></i> Select Start Path:</label>
                    <select asp-for="StartPath" class="form-select">
                        <option value="https://markhazleton.com">markhazleton.com</option>
                        <option value="https://webspark.markhazleton.com">webspark.markhazleton.com</option>
                        <option value="https://mechanicsofmotherhood.com">MechanicsOfMotherhood.com</option>
                        <option value="https://texecon.com">Texecon.com</option>
                    </select>
                </div>
                <div class="mb-3">
                    <label asp-for="MaxPagesCrawled" class="form-label"><i class="bi bi-layers"></i> Maximum Pages Crawled:</label>
                    <input asp-for="MaxPagesCrawled" class="form-control" type="number" min="1" max="900" />
                </div>
                <button type="submit" class="btn btn-primary" id="crawlButton"><i class="bi bi-play-circle"></i> Start Crawling</button>
            </form>
        </div>
    </div>

    <div class="text-center mt-4 loading-spinner d-none">
        <i class="bi bi-arrow-repeat fa-spin fa-3x"></i>
        <p class="mt-2">Crawling in progress...</p>
    </div>

    <div class="card mt-4">
        <div class="card-header">
            <h3 class="card-title mb-0"><i class="bi bi-info-circle"></i> Crawl Status</h3>
        </div>
        <div class="card-body">
            <p id="url-found"></p>
            @if (Model?.CrawlResults != null && Model?.CrawlResults?.Count > 0)
            {
                <div class="table-responsive">
                    <table class="table table-striped mt-4" id="crawlResults">
                        <thead class="table-dark">
                            <tr>
                                <th>Iteration</th>
                                <th>Request Path</th>
                                <th>Links Found</th>
                                <th>Completion Date</th>
                                <th>Elapsed Milliseconds</th>
                                <th>Status Code</th>
                                <th>Retries</th>
                                <th>Errors</th>
                            </tr>
                        </thead>
                        <tbody>
                            @foreach (var item in Model.CrawlResults)
                            {
                                <tr>
                                    <td>@Html.DisplayFor(modelItem => item.Iteration)</td>
                                    <td><a href="@item.RequestPath" target="_blank" class="text-decoration-none"><i class="bi bi-box-arrow-up-right"></i> @item.RequestPath</a></td>
                                    <td>@Html.DisplayFor(modelItem => item.CrawlLinks.Count)</td>
                                    <td>@Html.DisplayFor(modelItem => item.CompletionDate)</td>
                                    <td>@Html.DisplayFor(modelItem => item.ElapsedMilliseconds)</td>
                                    <td>@Html.DisplayFor(modelItem => item.StatusCode)</td>
                                    <td>@Html.DisplayFor(modelItem => item.Retries)</td>
                                    <td>@string.Join(", ", item.ErrorList)</td>
                                </tr>
                            }
                        </tbody>
                    </table>
                </div>
                <div class="mt-4">
                    <h3>Sitemap XML</h3>
                    <div class="position-relative border p-3 bg-light rounded overflow-auto" style="max-height: 300px;">
                        <pre class="language-xml mb-0">
                          <code class="language-xml">
                            @Html.Raw(Model.Sitemap)
                          </code>
                        </pre>
                        <div class="d-flex justify-content-end mt-3">
                            <button class="btn btn-secondary btn-sm me-2" onclick="copyToClipboard()">
                                <i class="bi bi-clipboard"></i> Copy
                            </button>
                            <button class="btn btn-primary btn-sm" onclick="saveAsFile()">
                                <i class="bi bi-download"></i> Save as sitemap.xml
                            </button>
                        </div>
                    </div>
                </div>
            }
        </div>
    </div>
</div>

<section class="container my-5">
    <h1 class="display-4 text-center mb-4"><i class="bi bi-search"></i> Site Crawler Documentation</h1>

    <div class="card shadow-sm mb-4">
        <div class="card-header bg-primary text-white">
            <h2 class="card-title mb-0"><i class="bi bi-info-circle"></i> Overview</h2>
        </div>
        <div class="card-body">
            <p>The Site Crawler is an ASP.NET Core application designed to crawl websites, gather URLs, and generate sitemaps. It uses <strong>HttpClient</strong> for making web requests and <strong>SignalR</strong> for providing real-time updates to the users. The crawler is built with scalability, efficiency, and resilience in mind, following best practices for modern web crawling.</p>
        </div>
    </div>

    <div class="card shadow-sm mb-4">
        <div class="card-header bg-secondary text-white">
            <h2 class="card-title mb-0"><i class="bi bi-gear-fill"></i> Key Features</h2>
        </div>
        <div class="card-body">
            <ul class="list-group list-group-flush">
                <li class="list-group-item"><i class="bi bi-lightning-charge-fill text-warning"></i> <strong>Asynchronous Crawling:</strong> Utilizes <code>async/await</code> for non-blocking I/O operations, making it efficient in handling large numbers of web requests.</li>
                <li class="list-group-item"><i class="bi bi-recycle text-success"></i> <strong>Error Handling and Resilience:</strong> Implements retries and fallback mechanisms with the Polly library to gracefully manage transient faults and network issues.</li>
                <li class="list-group-item"><i class="bi bi-broadcast text-info"></i> <strong>Real-Time Feedback:</strong> Uses SignalR to provide live updates on crawling progress, enhancing user experience with immediate feedback.</li>
                <li class="list-group-item"><i class="bi bi-shield-lock-fill text-danger"></i> <strong>Robust Security:</strong> Complies with web standards and respects <code>robots.txt</code> directives, ensuring that crawling is performed ethically and legally.</li>
            </ul>
        </div>
    </div>

    <div class="card shadow-sm mb-4">
        <div class="card-header bg-info text-white">
            <h2 class="card-title mb-0"><i class="bi bi-code-slash"></i> Implementation Details</h2>
        </div>
        <div class="card-body">
            <h3>1. <i class="bi bi-diagram-2-fill"></i> Architecture</h3>
            <p>The Site Crawler consists of several components:</p>
            <ul>
                <li><strong>SiteCrawler Class:</strong> The core component responsible for initiating HTTP requests, parsing results, and managing the crawl queue.</li>
                <li><strong>CrawlDomainController:</strong> The MVC controller that handles user interactions and triggers crawling operations.</li>
                <li><strong>SignalR Hub:</strong> Facilitates real-time communication between the server and clients to update users on crawl progress.</li>
            </ul>

            <h3>2. <i class="bi bi-link-45deg"></i> Key Methods</h3>
            <p>The <code>SiteCrawler</code> class includes key methods such as:</p>
            <ul>
                <li><code>CrawlAsync</code>: Initiates the crawling process, manages the queue of URLs, and handles the crawling depth and limits.</li>
                <li><code>CrawlPageAsync</code>: Crawls individual pages, handles HTTP responses, and manages error states.</li>
                <li><code>GenerateSitemapXml</code>: Generates a sitemap in XML format, adhering to the standard sitemap protocol.</li>
            </ul>
        </div>
    </div>

    <div class="card shadow-sm mb-4">
        <div class="card-header bg-success text-white">
            <h2 class="card-title mb-0"><i class="bi bi-lightbulb-fill"></i> Best Practices</h2>
        </div>
        <div class="card-body">
            <h3>1. <i class="bi bi-arrow-repeat"></i> Efficient Crawling</h3>
            <p>Optimize your crawling by setting sensible limits on depth and the number of pages. This helps prevent overloading your system and respects target websites' bandwidth.</p>

            <h3>2. <i class="bi bi-exclamation-triangle-fill"></i> Error Handling</h3>
            <p>Use comprehensive error handling strategies with detailed logging to diagnose and recover from failures effectively. Implement retries for transient errors using policies like exponential backoff.</p>

            <h3>3. <i class="bi bi-lock-fill"></i> Respect Robots.txt</h3>
            <p>Always check the <code>robots.txt</code> file of the target website to ensure that your crawler does not access restricted areas, which could lead to legal issues.</p>

            <h3>4. <i class="bi bi-people-fill"></i> Provide Real-Time Feedback</h3>
            <p>Enhance user experience by utilizing SignalR to send real-time updates on the crawl progress. This keeps users informed and engaged throughout the process.</p>
        </div>
    </div>

    <div class="card shadow-sm mb-4">
        <div class="card-header bg-warning text-dark">
            <h2 class="card-title mb-0"><i class="bi bi-tools"></i> Setup and Configuration</h2>
        </div>
        <div class="card-body">
            <h3>1. <i class="bi bi-box-arrow-in-right"></i> Installation</h3>
            <p>Add the necessary packages to your ASP.NET Core project:</p>
            <pre class="language-bash"><code class="language-bash">dotnet add package Microsoft.AspNetCore.SignalR
dotnet add package Microsoft.Extensions.Http
dotnet add package Polly</code></pre>

            <h3>2. <i class="bi bi-gear"></i> Configuration</h3>
            <p>Configure the HttpClient and SignalR in your <code>Program.cs</code>:</p>
            <pre class="language-csharp"><code class="language-csharp">
var builder = WebApplication.CreateBuilder(args);

// Configure services
builder.Services.AddHttpClient();
builder.Services.AddSignalR();
builder.Services.AddScoped<SiteCrawler>();

var app = builder.Build();

// Configure middleware
app.UseRouting();
app.UseEndpoints(endpoints =>
{
    endpoints.MapHub<CrawlHub>("/crawlHub");
    endpoints.MapControllers();
});

app.Run();
            </code></pre>
        </div>
    </div>

    <div class="alert alert-info text-center mt-4" role="alert">
        <i class="bi bi-lightning-charge-fill"></i> Enhance your web applications with the Site Crawler by following these guidelines and best practices for optimal performance and compliance!
    </div>
</section>

@section Scripts {
    <script>
        $(document).ready(function() {
            var table = $('#crawlResults').DataTable({
                paging: false,
                scrollY: '40vh',
                scrollCollapse: true,
                stateSave: false,
                colReorder: true
            });
        });
    </script>
}

@section PageScripts {
    <script src="https://cdnjs.cloudflare.com/ajax/libs/microsoft-signalr/5.0.7/signalr.min.js"></script>
    <script>
        // Establish SignalR connection
        const connection = new signalR.HubConnectionBuilder()
            .withUrl("/crawlHub")
            .build();

        // Variable to track crawling status
        let isCrawling = false;

        connection.on("UrlFound", function(numUrlsFound) {
            const found = document.querySelector("p#url-found");
            found.innerHTML = numUrlsFound + "<br/>";
            const spinner = document.querySelector(".loading-spinner");
            const results = document.querySelector("table#crawlResults");
            
            if (isCrawling) {
                spinner.classList.remove("d-none");
                if (results) {
                    results.classList.add("d-none");
                }
            } else {
                spinner.classList.add("d-none");
                if (results) {
                    results.classList.remove("d-none");
                }
            }
        });

        // Start the connection
        connection.start().catch(function(err) {
            return console.error(err.toString());
        });
    </script>
    <script>
        // Function to copy the sitemap XML to the clipboard
        function copyToClipboard() {
            const codeElement = document.querySelector('.language-xml code');
            navigator.clipboard.writeText(codeElement.textContent).then(() => {
                alert('Sitemap XML copied to clipboard!');
            }).catch(err => {
                console.error('Failed to copy text: ', err);
            });
        }

        // Function to save the sitemap XML as a file
        function saveAsFile() {
            const blob = new Blob([document.querySelector('.language-xml code').textContent], { type: 'application/xml' });
            const link = document.createElement('a');
            link.href = URL.createObjectURL(blob);
            link.download = 'sitemap.xml';
            link.click();
            URL.revokeObjectURL(link.href);
        }
    </script>
}
